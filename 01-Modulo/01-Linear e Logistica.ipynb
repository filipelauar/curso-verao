{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear e Logistica.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GBvUmcHxonaS"},"source":["# Aprendizado Profundo - UFMG\n","\n","## Preâmbulo\n","\n","O código abaixo consiste dos imports comuns. Além do mais, configuramos as imagens para ficar de um tamanho aceitável e criamos algumas funções auxiliares. No geral, você pode ignorar a próxima célula."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W-iRUIgyouKm","colab":{}},"source":["# -*- coding: utf8\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","\n","plt.rcParams['figure.figsize']  = (18, 10)\n","plt.rcParams['axes.labelsize']  = 20\n","plt.rcParams['axes.titlesize']  = 20\n","plt.rcParams['legend.fontsize'] = 20\n","plt.rcParams['xtick.labelsize'] = 20\n","plt.rcParams['ytick.labelsize'] = 20\n","plt.rcParams['lines.linewidth'] = 4\n","torch.set_printoptions(precision=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r5J4r1Joonac","colab":{}},"source":["plt.ion()\n","\n","plt.style.use('seaborn-colorblind')\n","plt.rcParams['figure.figsize']  = (12, 8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r1hxSo7nonag"},"source":["Para testar o resultado dos seus algoritmos vamos usar o módulo testing do numpy."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cKAN6JuJonah","colab":{}},"source":["from numpy.testing import assert_equal\n","from numpy.testing import assert_almost_equal\n","from numpy.testing import assert_array_almost_equal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8FVe1G2wonak"},"source":["## Regressão Linear e Logística from Scratch\n","\n","Para brincar um pouco mais com essa diferenciação automatica, presente em frameworks como pytorch e mxnet, nesta aula vamos implementar a regressão linear e logística do zero. Vamos fazer duas versões de cada:\n","\n","1. Derivando na mão, não é complicado.\n","2. Derivando com autograd de pytorch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"waN0vx9fonal"},"source":["## Conjunto de Problemas 1: Mais Derivadas"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uK0g7SKdonam"},"source":["Antes de entrar na regressão, vamos brincar um pouco de derivadas dentro de funções. Dado dois números `x` e `y`, implemente a função `log_exp` que retorna:\n","\n","$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OsgyDlAHpBdZ","colab":{}},"source":["def log_exp(x, y):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EnoHouBPonap"},"source":["1. Abaixo vamos testar o seu código com algumas entradas simples."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2fd-sln-w2Op","colab":{}},"source":["x, y = torch.tensor([2.0]), torch.tensor([3.0])\n","z = log_exp(x, y)\n","z"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LQNEAUNoonau","colab":{}},"source":["# Teste. Não apague\n","assert_almost_equal(1.31326175, z.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TdhvPzMxonax"},"source":["2. A função a seguir computa $\\partial z/\\partial x$ e $\\partial z/\\partial y$ usando `autograd`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lHBQrZD7qsYN","colab":{}},"source":["# O argumento funcao_forward é uma função python. Será a sua log_exp.\n","# A ideia aqui é deixar claro a ideia de forward e backward propagation, depois\n","# de avaliar a função chamamos backward e temos as derivadas.\n","def grad(funcao_forward, x, y):\n","    x.requires_grad_(True)\n","    y.requires_grad_(True)\n","    z = funcao_forward(x, y)\n","    z.backward()\n","    return x.grad, y.grad"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-WFpkBMvona2"},"source":["Testando"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vMVfDIKQrAJL","colab":{}},"source":["x, y = torch.tensor([2.0], dtype = torch.double) ,torch.tensor([3.0], dtype = torch.double)\n","dx, dy = grad(log_exp, x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oIkTfFH0snPD","colab":{}},"source":["assert_almost_equal(-0.7310586, dx.numpy())\n","assert_almost_equal(0.7310586, dy.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1oNpz1wIona9"},"source":["4. Agora teste com números maiores, algum problema?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0qEsHY5VsuH_","colab":{}},"source":["x, y = torch.tensor([400.0]).double() ,torch.tensor([800.0]).double()\n","grad(log_exp, x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XGUiPmJLonbB"},"source":["5. Pense um pouco sobre o motivo do erro acima. Usando as propriedade de logaritmos, é possível fazer uma função mais estável. Abaixo segue a implementação da mesma. O problema aqui é que o exponencial \"explode\" quando x ou y são muito grandes. Este [link](http://www.wolframalpha.com/input/?i=log[e%5Ex+%2F+[e%5Ex+%2B+e%5Ey]]) pode ajudar."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rnj8B4EyttvL","colab":{}},"source":["x, y = torch.tensor([400.0], dtype = torch.double) ,torch.tensor([800.0], dtype = torch.double)\n","def stable_log_exp(x, y):\n","    return torch.log(torch.exp(y-x))\n","\n","dx, dy = grad(stable_log_exp, x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B9QIVgN1onbF","colab":{}},"source":["stable_log_exp(x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aVWzzA4-t9J2","colab":{}},"source":["# Teste. Não apague\n","assert_equal(-1, dx.numpy())\n","assert_equal(1, dy.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"esV2XR7jonbL"},"source":["O exemplo acima mostra um pouco de problemas de estabilidade númerica. Às vezes é melhor usar versões alternativas de funções. Isto vai ocorrer quando você ver vários nans na sua frente :-) Claro, estamos assumindo que existe uma outra função equivalente que é mais estável para o computador."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3qI91TX7onbM"},"source":["## Conjunto de Problemas 2: Regressão Linear\n","\n","Agora, vamos explorar uma regressão linear. Embora não vamos fazer uso da logexp acima, a ideia de derivar parcialmente dentro de funções pode nos ajudar. Lembrando da regressão linear, inicialmente temos um conjunto observações representadas como tuplas $(\\mathbf{x}_i, y_i)$. Aqui, $\\mathbf{x}_i$ é um vetor de atributos. Vamo forçar $\\mathbf{x}_{i0} = 1$, capturando assim o intercepto. Além do mais, $\\mathbf{x}_{ij}$ quando $j\\neq 0$, são os outros atributos de entrada. $y_i$ um valor real representando uma resposta. Nossa regressão visa capturar:\n","\n","$$y_i = 1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots + \\theta_k x_{if}$$\n","\n","Lembrando da regressão linear multivariada, podemos representar as equações como uma multiplicação de uma matriz com um vetor\n","\n","![](./figs/linear.png)\n","\n","7. Crie uma função de previsao. A mesma recebe uma matrix $\\mathbf{X}$ e um vetor de parâmetros $\\theta$. Sua função deve retornar um vetor de previsões para cada linha de $\\mathbf{X}$. Não use nenhum laço!!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HveutorucYMJ","colab":{}},"source":["def previsao(X, theta):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iNRX8E0MonbP"},"source":["**Erros quadrados**. Para aprender os parâmetros ótimos da regressão linear, precisamos fazer uso de um modelo de erros quadrados. Em particular nosso objetico é aprender os parâmetros que minimizam a função:\n","\n","$$L(\\mathbf{\\theta}) = n^{-1} \\sum_i ({\\hat{y}_i - y_i})^2$$\n","\n","Onde $\\hat{y}_i$ é uma previsão (vêm da sua função python acima). $y_i$ é o valor real dos dados.\n","\n","8. Implemente uma função para a média dos os erros quadrados."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sS7y5Wb2cusy","colab":{}},"source":["def media_erros_quadrados(X, theta, y):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EVqJCucconbT"},"source":["9. Agora, crie uma função que deriva o erro acima. A função deve usar o autograd do pytorch (função backward). Lembre-se que temos um vetor de parâmetros $\\theta$. Por sorte, você pode fazer derivadas de tais vetores também."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_ao1G8uSc8Bp","colab":{}},"source":["def derivada_torch(X, theta, y): #Lembre que o theta passado devera ser uma variavel com requires_grad == True\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rnWTcJubonba"},"source":["10. Esta versão não usa autograd. As derivadas são implementadas do zero. Nos [slides](https://docs.google.com/presentation/d/1bz3G3fEohNtvERKSDtThfGPYOjF83Fzy2yHPdCAlFZw/edit#slide=id.g584f66d2ae_3_74) do link explicamos como fazer tal operação."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DBpOTZjPdLrq","colab":{}},"source":["def derivada_navera(X, theta, y):\n","    return ((torch.matmul(X, theta) - y) * X.T).mean(axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tj5LkKLzonbf"},"source":["11. Por fim, otimize sua função usando o algoritmo de gradiente descendente abaixo."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j-9buFPGfVPG","colab":{}},"source":["def gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001, max_iter=10000):\n","    '''\n","    Executa Gradiente Descendente. Aqui:\n","    \n","    Parâmetros\n","    ----------\n","    d_fun : é uma função de derivadas\n","    loss_fun : é uma função de perda\n","    X : é um vetor de fatores explanatórios.\n","        Copie seu código de intercepto da primeira aula.\n","        para adicionar o intercepto em X.\n","    y : é a resposta\n","    lambda : é a taxa de aprendizad\n","    tol : é a tolerância, define quando o algoritmo vai parar.\n","    max_ter : é a segunda forma de parada, mesmo sem convergir\n","              paramos depois de max_iter iterações.\n","    '''\n","    theta = torch.randn(X.shape[1])\n","    theta.requires_grad_(True)\n","    \n","    print('Iter {}; theta = '.format(0), theta)\n","    \n","    old_err_sq = np.inf\n","    i = 0\n","    while True:\n","        # Computar as derivadas\n","                \n","        theta.requires_grad_(True)  #Necessario pois ao final, theta recebe o theta novo, que não tem requires_grad == True\n","        grad = d_fun(X,theta,y)\n","\n","        # Atualizar\n","        with torch.no_grad(): \n","            theta_novo = theta - lambda_ * grad\n","        \n","        #Parar quando o erro convergir\n","        err_sq = loss_fun(X, theta, y)\n","        if torch.abs(old_err_sq - err_sq) <= tol:\n","            break\n","        \n","         #Atualizar parâmetros e erro\n","        theta = theta_novo\n","        old_err_sq = err_sq\n","        \n","        # Informação de debug\n","        print('Iter {}; theta = '.format(i+1), theta)\n","        i += 1\n","        if i == max_iter:\n","            break\n","        \n","    return theta"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RVWvK5mffuqd","colab":{}},"source":["# Para testes, não apague!!!\n","X = torch.zeros(1000, 2)\n","X[:, 0] = 1.0\n","X[:, 1] = torch.randn(1000)\n","\n","theta_0_real = 7.0\n","theta_1_real = 9.0\n","y = theta_0_real + theta_1_real * X[:, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IxE4o8ALonbm","colab":{}},"source":["# Testando com derivada autograd. Sua função deve retornar algo perto de 7 e 9\n","theta = gd(derivada_torch, media_erros_quadrados, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-YZwD4GZonbo","colab":{}},"source":["theta"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p9DuRmmjonbr","colab":{}},"source":["# Testando com derivada manual. Sua função deve retornar algo perto de 7 e 9\n","theta = gd(derivada_navera, media_erros_quadrados, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z6SbuwB7onbt","colab":{}},"source":["theta"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BQY5_lJvonbv"},"source":["12. Altere a função de Gradiente Descendente para funcionar com minibatches. Em outras palavras, não compute o erro usando todos os dados de X. Use um `slice` de tamanho do minibatch. Uma ideia é seguir o pseudocódigo abaixo.\n","\n","```python\n","index = np.arange(len(X))\n","while True:\n","    minib = np.random.choice(index, minibatchsize) # aqui estou usando numpy para selection minibatch elementos\n","    X_batch = X[minib]\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LOyf2F26onbv","scrolled":true,"colab":{}},"source":["# Exemplo abaixo\n","index = np.arange(len(X))\n","mb = np.random.choice(index, 50)\n","print(mb)\n","X[mb]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbssWxyCWBNY","colab_type":"code","colab":{}},"source":["def minibatch_gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001,\n","                 max_iter=10000, batch_size=10):\n","    '''\n","    Executa Gradiente Descendente. Aqui:\n","    \n","    Parâmetros\n","    ----------\n","    d_fun : é uma função de derivadas\n","    loss_fun : é uma função de perda\n","    X : é um vetor de fatores explanatórios.\n","        Copie seu código de intercepto da primeira aula.\n","        para adicionar o intercepto em X.\n","    y : é a resposta\n","    lambda : é a taxa de aprendizad\n","    tol : é a tolerância, define quando o algoritmo vai parar.\n","    max_ter : é a segunda forma de parada, mesmo sem convergir\n","              paramos depois de max_iter iterações.\n","    batch_size : tamanho do batch\n","    '''\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q5gzuUW4onb3","colab":{}},"source":["# Sua função deve retornar algo perto de 7 e 9\n","minibatch_gd(derivada_torch, media_erros_quadrados, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"khoTY_BZonb7"},"source":["## Conjunto de Problemas 3: Logistic from Scratch"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"puVaL8Kuonb8"},"source":["12. Repita o mesmo processo para a Logística. Lembrando que a mesma tem a seguinte forma:\n","\n","$$f(x_i) = \\frac{1}{1 + e^{-(1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots \\theta_k x_{ik})}}$$\n","\n","Implemente a função logística."]},{"cell_type":"code","metadata":{"id":"9gMYzjfDWBNh","colab_type":"code","colab":{}},"source":["def logistic(X, theta):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-AsqWqHWBNk","colab_type":"code","colab":{}},"source":["# testes, não apague!\n","X_teste = torch.randn(1000, 20000)\n","theta = torch.randn(20000)\n","y_hat_teste = logistic(X_teste, theta)\n","assert_equal(True, (y_hat_teste >= 0).numpy().all())\n","assert_equal(True, (y_hat_teste <= 1).numpy().all())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xcOzTSYqoncA"},"source":["Usando a logística acima implemente uma função logistica_prever que retorna 0 ou 1. Use o limiar dado na função. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7IzJAoDYoncB","colab":{}},"source":["def logistica_prever(X, theta, limiar=0.5):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeNGI7IFWBNs","colab_type":"code","colab":{}},"source":["# testes, não apague!\n","X_teste = torch.randn(1000, 20000)\n","theta = torch.randn(20000)\n","y_hat_teste = logistica_prever(X_teste, theta)\n","for yi in y_hat_teste.numpy():\n","    assert(yi in {0, 1})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"34WQX2AloncI"},"source":["Agora, implemente uma função de entropia cruzada da logística. A mesma, é proporcional ao inverso da verossimilhança. Para entender a derivação entre as duas faça uso dos [Slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing). \n","\n","Sendo:\n","\n","$$ll(x_i,y_i~|~\\theta) = y_i \\log f(x_i\\theta) + (1-y_i) \\log (1-f(x_i\\theta))$$\n","\n","A verossimilhança para uma observação. A entropia cruzada é a media da negação do termo para todos os exemplos:\n","\n","$$L(\\theta) = -n^{-1}\\sum_i \\big((1-y_i)\\log (1-f_{\\theta}(x_i)) + y_i\\log (f_{\\theta}(x_i))\\big)$$\n","\n","`dica: use torch.clamp(logistic(X, theta), min = 0.001, max = 0.999)`. A função remove os 0 e 1s da logistic, evitando assim o valor log(0)."]},{"cell_type":"code","metadata":{"id":"z5XRS87bWBNw","colab_type":"code","colab":{}},"source":["def cross_entropy_mean(X, theta, y):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MC_fPXj0oncM","colab":{}},"source":["# testes, não apague!\n","from sklearn import datasets\n","state = np.random.seed(20190187)\n","\n","X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2)\n","X = torch.tensor(X).double()\n","y = torch.tensor(y).double()\n","\n","for _ in range(100):\n","    theta = torch.randn(2,1).double()\n","    assert(cross_entropy_mean(X, theta, y) >= 0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_1E_BSkhoncO"},"source":["Agora implemente a derivada."]},{"cell_type":"code","metadata":{"id":"iJqaZncSWBN2","colab_type":"code","colab":{}},"source":["def derivada_torch_logit(X, theta, y):\n","    # implemente\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D4kELMG4oncS"},"source":["A partir daqui basta executar código."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QZP6mlFIoncT","colab":{}},"source":["plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], s=80, edgecolors='k')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XlRLH5sMoncW"},"source":["13. Assim como foi feito na primeira aula, abaixo testamos o seu código com um toy dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_mY_Dfj8oncW","colab":{}},"source":["# Use essa função antes de executar o GD\n","def add_intercept(X):\n","    Xn = torch.zeros(X.shape[0], X.shape[1] + 1).double()\n","    Xn[:, 0]  = 1.0\n","    Xn[:, 1:] = X\n","    return Xn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jT4OCJwYoncb","colab":{}},"source":["Xn = add_intercept(X)\n","theta = minibatch_gd(derivada_torch_logit, cross_entropy_mean, Xn, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"47Iaopm_onch","colab":{}},"source":["y_p = logistica_prever(Xn, theta)\n","print(y == y_p.int().double())\n","print((y == y_p.int().double()).double().mean())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8At5L06PWBOS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kk-FW0KdWBOV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}